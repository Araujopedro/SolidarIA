# -*- coding: utf-8 -*-
"""GS2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZVDdg03OdZ6-BE-qZR8FFkkGnMh-lbtg
"""

pip install pandas numpy seaborn

import kagglehub

# Download latest version
path = kagglehub.dataset_download("juliotorniero/civil-defense-occurrences")

print("Path to dataset files:", path)

import pandas as  pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

file_path = f"{path}/civil_defense_br.csv"


df = pd.read_csv(file_path)


print(df.head())

df.info()

features = [
    'Dead', 'Homeless', 'Homes_destroyed', 'Public_health_places_damaged',
    'Industry_loss', 'Agriculture_loss', 'Infraestructure_value', 'Public_use installations_value'
]

# Exemplo de saída: necessidade de água potável (Potable_water_expedenture)
target = 'Potable_water_expedenture'

X = df[features]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criar e treinar o modelo
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Fazer previsões
y_pred = model.predict(X_test)

# Avaliar o modelo
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"R²: {r2}")

# Importância das features
feature_importances = pd.DataFrame({
    'Feature': features,
    'Importance': model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print(feature_importances)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Carregar o dataset
data = pd.read_csv('civil_defense_br.csv')

# Selecionar as features mais relevantes
# Incluímos novamente as features que antes tiveram baixa importância para testar novas combinações
features = [
    'Agriculture_loss', 'Infraestructure_value', 'Public_health_places_damaged',
    'Homeless', 'Industry_loss', 'Public_use installations_value'
]

# Exemplo de saída: necessidade de água potável (Potable_water_expedenture)
target = 'Potable_water_expedenture'

# Dividir os dados em treino e teste
X = data[features]
y = data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ajustar escala para modelos sensíveis a valores extremos
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Criar e treinar o modelo com Random Forest
model = RandomForestRegressor(random_state=42, n_estimators=500, max_depth=10)
model.fit(X_train, y_train)

# Fazer previsões
y_pred = model.predict(X_test)

# Avaliar o modelo
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"R²: {r2}")

# Importância das features
feature_importances = pd.DataFrame({
    'Feature': features,
    'Importance': model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print(feature_importances)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Carregar o dataset
data = pd.read_csv('civil_defense_br.csv')

# Selecionar as features mais relevantes
# Incluímos novamente as features que antes tiveram baixa importância para testar novas combinações
features = [
    'Agriculture_loss', 'Infraestructure_value', 'Public_health_places_damaged',
    'Homeless', 'Industry_loss', 'Public_use installations_value'
]

# Exemplo de saída: necessidade de água potável (Potable_water_expedenture)
target = 'Potable_water_expedenture'

# Dividir os dados em treino e teste
X = data[features]
y = data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ajustar escala para modelos sensíveis a valores extremos
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Criar e treinar o modelo com XGBoost
model = XGBRegressor(random_state=42, n_estimators=500, max_depth=10, learning_rate=0.1)
model.fit(X_train, y_train)

# Fazer previsões
y_pred = model.predict(X_test)

# Avaliar o modelo
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse}")
print(f"R²: {r2}")

# Importância das features
feature_importances = pd.DataFrame({
    'Feature': features,
    'Importance': model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print(feature_importances)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, mean_squared_error
import joblib
from datetime import datetime

class DisasterDonationML:
    def __init__(self):
        self.need_predictor = None
        self.impact_predictor = None
        self.recommendation_model = None
        self.scalers = {}
        self.encoders = {}

    def prepare_features(self, df):
        """Prepara features para os modelos"""
        # Criar features temporais
        df['Date'] = pd.to_datetime(df['Date'])
        df['Year'] = df['Date'].dt.year
        df['Month'] = df['Date'].dt.month
        df['Season'] = df['Month'].apply(self._get_season)

        # Feature de densidade populacional relativa
        df['Pop_Density_Category'] = pd.cut(df['Population'],
                                           bins=5, labels=['Muito_Baixa', 'Baixa', 'Media', 'Alta', 'Muito_Alta'])

        # Extrair tipo principal do desastre
        df['Disaster_Main_Type'] = df['Complaint'].str.extract(r'(\d{5})')[0]

        # Feature de impacto total
        df['Total_Affected'] = (df['Dead'] + df['Enjuried'] +
                               df['Homeless'] + df['Displaced'] + df['Other_affected'])

        # Categoria de urgência baseada em mortos e feridos
        df['Urgency_Level'] = self._categorize_urgency(df)

        return df

    def _get_season(self, month):
        """Converte mês em estação do ano (hemisfério sul)"""
        if month in [12, 1, 2]:
            return 'Verao'
        elif month in [3, 4, 5]:
            return 'Outono'
        elif month in [6, 7, 8]:
            return 'Inverno'
        else:
            return 'Primavera'

    def _categorize_urgency(self, df):
        """Categoriza urgência baseada em mortos e feridos"""
        conditions = [
            (df['Dead'] > 0) | (df['Enjuried'] > 50),
            (df['Enjuried'] > 0) | (df['Homeless'] > 100),
            (df['Displaced'] > 500),
        ]
        choices = ['Critica', 'Alta', 'Media']
        return np.select(conditions, choices, default='Baixa')

    def create_donation_categories(self, df):
        """Cria categorias de doação baseadas no tipo de desastre"""
        donation_map = {
            '15110': 'Medicamentos_EPIs',  # Doenças infecciosas
            '14110': 'Agua_Alimentos',     # Estiagem
            '14120': 'Agua_Alimentos',     # Seca
            '13214': 'Roupas_Abrigo',      # Tempestades/Chuvas
            '13215': 'Roupas_Abrigo',      # Vendaval
            '12100': 'Roupas_Abrigo',      # Inundações
            '12300': 'Roupas_Abrigo',      # Alagamentos
            '12200': 'Roupas_Abrigo',      # Enxurradas
            '14132': 'Respiratorios',      # Incêndio - qualidade do ar
            '14131': 'Meio_Ambiente',      # Incêndio - áreas protegidas
        }

        df['Donation_Category'] = df['Disaster_Main_Type'].map(donation_map)
        df['Donation_Category'] = df['Donation_Category'].fillna('Geral')
        return df

    def train_need_prediction_model(self, df):
        """Treina modelo para predizer tipo de doação necessária"""
        # Preparar dados
        df_processed = self.prepare_features(df)
        df_processed = self.create_donation_categories(df_processed)

        # Features para predição
        feature_cols = ['Population', 'Year', 'Month', 'Season', 'State', 'Urgency_Level']

        # Encoding de variáveis categóricas
        for col in ['Season', 'State', 'Urgency_Level']:
            if col not in self.encoders:
                self.encoders[col] = LabelEncoder()
                df_processed[f'{col}_encoded'] = self.encoders[col].fit_transform(df_processed[col].astype(str))
            else:
                df_processed[f'{col}_encoded'] = self.encoders[col].transform(df_processed[col].astype(str))

        # Preparar dados de treino
        X_features = ['Population', 'Year', 'Month', 'Season_encoded', 'State_encoded', 'Urgency_Level_encoded']
        X = df_processed[X_features].fillna(0)
        y = df_processed['Donation_Category'].fillna('Geral')

        # Encoding do target
        if 'donation_category' not in self.encoders:
            self.encoders['donation_category'] = LabelEncoder()
            y_encoded = self.encoders['donation_category'].fit_transform(y)

        # Split dados
        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

        # Treinar modelo
        self.need_predictor = RandomForestClassifier(n_estimators=100, random_state=42)
        self.need_predictor.fit(X_train, y_train)

        # Avaliar modelo
        y_pred = self.need_predictor.predict(X_test)
        y_test_labels = self.encoders['donation_category'].inverse_transform(y_test)
        y_pred_labels = self.encoders['donation_category'].inverse_transform(y_pred)

        print("Relatório do Modelo de Predição de Necessidades:")
        print(classification_report(y_test_labels, y_pred_labels))

        return self.need_predictor

    def train_impact_prediction_model(self, df):
        """Treina modelo para predizer impacto/severidade"""
        df_processed = self.prepare_features(df)

        # Features para predição de impacto
        feature_cols = ['Population', 'Year', 'Month']

        # Encoding das variáveis categóricas
        for col in ['Season', 'State']:
            col_encoded = f'{col}_encoded'
            if col not in self.encoders:
                self.encoders[col] = LabelEncoder()
            df_processed[col_encoded] = self.encoders[col].fit_transform(df_processed[col].astype(str))
            feature_cols.append(col_encoded)

        # Preparar dados
        X = df_processed[feature_cols].fillna(0)
        y = df_processed['Total_Affected'].fillna(0)

        # Normalização
        if 'impact_scaler' not in self.scalers:
            self.scalers['impact_scaler'] = StandardScaler()
            X_scaled = self.scalers['impact_scaler'].fit_transform(X)

        # Split dados
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

        # Treinar modelo
        self.impact_predictor = GradientBoostingRegressor(n_estimators=100, random_state=42)
        self.impact_predictor.fit(X_train, y_train)

        # Avaliar modelo
        y_pred = self.impact_predictor.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)

        print(f"\nRelatório do Modelo de Predição de Impacto:")
        print(f"RMSE: {rmse:.2f}")
        print(f"Média de pessoas afetadas: {y.mean():.2f}")

        return self.impact_predictor

    def predict_donation_needs(self, population, year, month, season, state, urgency_level):
        """Prediz que tipo de doação é necessária"""
        if not self.need_predictor:
            raise ValueError("Modelo de predição de necessidades não foi treinado")

        # Preparar input
        season_encoded = self.encoders['Season'].transform([season])[0]
        state_encoded = self.encoders['State'].transform([state])[0]
        urgency_encoded = self.encoders['Urgency_Level'].transform([urgency_level])[0]

        X_input = np.array([[population, year, month, season_encoded, state_encoded, urgency_encoded]])

        # Predição
        prediction = self.need_predictor.predict(X_input)[0]
        donation_type = self.encoders['donation_category'].inverse_transform([prediction])[0]

        # Confiança da predição
        probabilities = self.need_predictor.predict_proba(X_input)[0]
        confidence = max(probabilities)

        return {
            'donation_type': donation_type,
            'confidence': confidence,
            'all_probabilities': dict(zip(
                self.encoders['donation_category'].classes_,
                probabilities
            ))
        }

    def predict_impact(self, population, year, month, season, state):
        """Prediz o impacto esperado (número de pessoas afetadas)"""
        if not self.impact_predictor:
            raise ValueError("Modelo de predição de impacto não foi treinado")

        # Preparar input
        season_encoded = self.encoders['Season'].transform([season])[0]
        state_encoded = self.encoders['State'].transform([state])[0]

        X_input = np.array([[population, year, month, season_encoded, state_encoded]])
        X_input_scaled = self.scalers['impact_scaler'].transform(X_input)

        # Predição
        predicted_impact = self.impact_predictor.predict(X_input_scaled)[0]

        return max(0, int(predicted_impact))  # Não pode ser negativo

    def get_donation_recommendations(self, disaster_type, location, severity):
        """Gera recomendações específicas de doação"""
        recommendations = {
            'Medicamentos_EPIs': [
                'Máscaras N95/PFF2',
                'Álcool em gel',
                'Medicamentos básicos (paracetamol, dipirona)',
                'Termômetros digitais',
                'Oxímetros'
            ],
            'Agua_Alimentos': [
                'Água potável (garrafões)',
                'Filtros de água',
                'Alimentos não perecíveis',
                'Ração para animais',
                'Sementes resistentes à seca'
            ],
            'Roupas_Abrigo': [
                'Roupas para todas as idades',
                'Cobertores e mantas',
                'Produtos de higiene pessoal',
                'Fraldas descartáveis',
                'Material de construção básico'
            ],
            'Respiratorios': [
                'Máscaras contra fumaça',
                'Medicamentos respiratórios',
                'Purificadores de ar',
                'Inaladores'
            ],
            'Meio_Ambiente': [
                'Equipamentos de combate a incêndio',
                'Mudas de árvores nativas',
                'Ferramentas de jardinagem'
            ]
        }

        return recommendations.get(disaster_type, ['Doações gerais conforme necessidade local'])

    def save_models(self, path='models/'):
        """Salva os modelos treinados"""
        import os
        os.makedirs(path, exist_ok=True)

        if self.need_predictor:
            joblib.dump(self.need_predictor, f'{path}need_predictor.pkl')
        if self.impact_predictor:
            joblib.dump(self.impact_predictor, f'{path}impact_predictor.pkl')

        joblib.dump(self.encoders, f'{path}encoders.pkl')
        joblib.dump(self.scalers, f'{path}scalers.pkl')

        print(f"Modelos salvos em {path}")

    def load_models(self, path='models/'):
        """Carrega os modelos salvos"""
        try:
            self.need_predictor = joblib.load(f'{path}need_predictor.pkl')
            self.impact_predictor = joblib.load(f'{path}impact_predictor.pkl')
            self.encoders = joblib.load(f'{path}encoders.pkl')
            self.scalers = joblib.load(f'{path}scalers.pkl')
            print("Modelos carregados com sucesso!")
        except FileNotFoundError as e:
            print(f"Erro ao carregar modelos: {e}")

# Exemplo de uso
if __name__ == "__main__":
    # Carregar dados
    df = pd.read_csv('civil_defense_br.csv')

    # Inicializar sistema ML
    ml_system = DisasterDonationML()

    # Treinar modelos
    print("Treinando modelo de predição de necessidades...")
    ml_system.train_need_prediction_model(df)

    print("\nTreinando modelo de predição de impacto...")
    ml_system.train_impact_prediction_model(df)

    # Exemplo de predição
    print("\n" + "="*50)
    print("EXEMPLO DE PREDIÇÃO:")

    prediction = ml_system.predict_donation_needs(
        population=50000,
        year=2024,
        month=3,
        season='Outono',
        state='MG',
        urgency_level='Alta'
    )

    print(f"Tipo de doação recomendada: {prediction['donation_type']}")
    print(f"Confiança: {prediction['confidence']:.2f}")

    impact = ml_system.predict_impact(
        population=50000,
        year=2024,
        month=3,
        season='Outono',
        state='MG'
    )

    print(f"Impacto estimado: {impact} pessoas afetadas")

    # Recomendações específicas
    recommendations = ml_system.get_donation_recommendations(
        prediction['donation_type'], 'MG', 'Alta'
    )

    print(f"\nRecomendações de doação:")
    for item in recommendations:
        print(f"• {item}")

    # Salvar modelos
    ml_system.save_models()